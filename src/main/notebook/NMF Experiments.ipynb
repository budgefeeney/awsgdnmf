{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Configuration"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "path = \"/Users/bryanfeeney/Desktop/SmallerDB-NoCJK-WithFeats\"\n",
      "%run /Users/bryanfeeney/Desktop/SmallerDB-NoCJK-WithFeats/dicts.py\n",
      "reconstructSparse=False"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Prelude"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import numpy.random as rd\n",
      "import scipy as sp\n",
      "import scipy.linalg as la\n",
      "import scipy.sparse as ssp\n",
      "import scipy.sparse.linalg as sla\n",
      "import pickle as pkl\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "\n",
      "import pickle as pkl\n",
      "import bottleneck\n",
      "\n",
      "rd.seed(0xC0FFEE)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Load and Clean Data\n",
      "\n",
      "Eliminate those users with very few tweets. Retain those users with very many tweets (these have already been checked and shown to be prolific users and news sites staffed by multiple users, rather than automated bots). This then compreses the dataset to be used for analysis"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open(path + \"/words.pkl\", 'rb') as f:\n",
      "    W = pkl.load(f)\n",
      "with open(path + \"/side.pkl\", 'rb') as f:\n",
      "    X = pkl.load(f)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##\u00a0Summary Statistics and Dictionaries"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Get document, feature, vocabulary etc counts. Make sure the numbers agree"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "(D,T) = W.shape\n",
      "(D_check,F) = X.shape\n",
      "\n",
      "assert D_check == D\n",
      "print (\"There are {:n} documents comprising {:n} tokens\".format(D, np.sum(W.data)))\n",
      "print (\"Vocab-Size = {:n}\".format(T))\n",
      "print (\"Total Feature Size = %d\" % F)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "There are 605319 documents comprising 5958065 tokens\n",
        "Vocab-Size = 79799\n",
        "Total Feature Size = 427\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The token vocabulary is divided into several token classes, these being *addresses*, *URLs*, *hashtags*, *emoticons*, *stocks* and standard *word-tokens* -- though not necessarily in that order. Initially this was done for the purposes of creating class-specific token-standardisation mechanisms (e.g. upper-case and trim stocks, lower-case, trim and stem word-tokens). However it also presents a multi-view learning problem, where in particular hash-tags and words present two different representations of the users topic.\n",
      "\n",
      "At this point we determine the start and end points for each token class in the matrix $W$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import itertools\n",
      "\n",
      "# The words structure is a list of lists. The position of a word in a\n",
      "# list determins its token value. We chain these together to create a\n",
      "# single token dictionary across all token classes.\n",
      "\n",
      "d = list(itertools.chain.from_iterable(words))\n",
      "\n",
      "print (\"The cumulative dictionary lengths is {:n}\".format(len(d)))\n",
      "\n",
      "# We then create a map of dictionary name to dictionary (i.e. list) and\n",
      "# also a table of dictionary lengths\n",
      "#\n",
      "dictNames = ['Addressee', 'URL', 'Text', 'Stock', 'Emoticon', 'Hashtag']\n",
      "dicts     = [ wordsUsername, wordsUrl, wordsToken, wordsStock, wordsEmoticon, wordsHashtag, ]\n",
      "dictLens  = dict (zip(dictNames, [len(w) for w in dicts]))\n",
      "\n",
      "print (\"The lengths of the dictionaries is:\")\n",
      "print (str(dictLens))\n",
      "\n",
      "# Find the stards and ends of the various token-specific dictionary\n",
      "#\n",
      "dictStarts = dict()\n",
      "acc = 0\n",
      "for k in dictNames:\n",
      "    dictStarts[k] = acc\n",
      "    acc += dictLens[k]\n",
      "\n",
      "dictEnds = dict()\n",
      "for i in range(len(dictNames) - 1):\n",
      "    dictEnds[dictNames[i]] = dictStarts[dictNames[i + 1]]\n",
      "dictEnds[dictNames[-1]] = len(d)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The cumulative dictionary lengths is 79800\n",
        "The lengths of the dictionaries is:\n",
        "{'Text': 63421, 'Hashtag': 13347, 'Emoticon': 593, 'URL': 1985, 'Addressee': 0, 'Stock': 454}\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we just print the minimum and maximum token counts"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "totalCount = np.squeeze(np.asarray (W.astype(np.int32).sum(axis=0)))\n",
      "\n",
      "totalCounts = dict()\n",
      "endExcl = 0\n",
      "for i in range(len(dicts)):\n",
      "    name = dictNames[i]\n",
      "    startIncl  = endExcl\n",
      "    endExcl   += len(dicts[i])\n",
      "    totalCounts[name] = totalCount[startIncl:endExcl]\n",
      "\n",
      "for k,v in totalCounts.items():\n",
      "    dict_k = dicts[dictNames.index(k)]\n",
      "    print (\"Min(%s): count = %d, token = %s\" % (k , v.min() if len(v) > 0 else -1, dict_k[v.argmin()] if len(v) > 0 else \"\"))\n",
      "    \n",
      "print(\"\")\n",
      "\n",
      "for k,v in totalCounts.items():\n",
      "    dict_k = dicts[dictNames.index(k)]\n",
      "    print (\"Max(%s): count = %d, token = %s\" % (k , v.max() if len(v) > 0 else -1, dict_k[v.argmax()] if len(v) > 0 else \"\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Min(Text): count = 0, token = 3\n",
        "Min(Hashtag): count = 0, token = #dfmchat\n",
        "Min(Emoticon): count = 0, token = \u00a7\n",
        "Min(URL): count = 0, token = http://www.youtube.com/watch?v=nYu6hfmVZk8\n",
        "Min(Addressee): count = -1, token = \n",
        "Min(Stock): count = 0, token = $nq\n",
        "\n",
        "Max(Text): count = 100764, token = you\n",
        "Max(Hashtag): count = 3902, token = #usopen\n",
        "Max(Emoticon): count = 44180, token = \u1f49B\n",
        "Max(URL): count = 176661, token = http://klout.com/user/louiebaur/topics?n=tw&v=plusK_thanks&i=266114\n",
        "Max(Addressee): count = -1, token = \n",
        "Max(Stock): count = 577, token = $aapl\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Feature Analysis\n",
      "\n",
      "Analysis of the features"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "feats"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "{'author': 0, 'month_of_year': 421, 'week_of_year': 398}"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(users)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "364"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##\u00a0Elimination of Users with Too Few Tweets"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_U = X[:,0:len(users)]\n",
      "tweetCountsPerUser = np.squeeze(np.asarray (X_U.astype(np.int32).sum(axis=0)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "silentUsers = np.where(tweetCountsPerUser < 100)[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rowsToDelete = []"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "silentUser = silentUsers[0]\n",
      "silentFilter = (X[:,silentUser] == 1).todense()\n",
      "for silentUser in silentUsers:\n",
      "    silentFilter |= (X[:,silentUser] == 1).todense()\n",
      "\n",
      "noisyFilter = ~silentFilter\n",
      "noisyFilter = np.squeeze(np.asarray(noisyFilter))\n",
      "\n",
      "X_noisy = X[noisyFilter,:]\n",
      "W_noisy = W[noisyFilter,:]\n",
      "\n",
      "print (\"Stripped {:n} rows\".format(X.shape[0] - X_noisy.shape[0]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Stripped 1667 rows\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##\u00a0Creation of Submatrices and Design Matrices\n",
      "\n",
      "We split the feature matrix $X$ and the document-term matrix $W$ into individual matrices containing separate views of data (user, week, month in the former, by token class in the latter). We then merge all these together into a single matrix denoted $Y$, and then create a noisy version of $Y$, which we denote $Z$, where an equal number of zeros and ones have been flipped."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_U = X_noisy[:,0:len(users)]\n",
      "X_W = X_noisy[:,feats['week_of_year']:(feats['month_of_year'] - 1)]\n",
      "X_M = X_noisy[:,feats['month_of_year']:X_noisy.shape[1]]\n",
      "\n",
      "# dictNames = ['Addressee', 'URL', 'Text', 'Stock', 'Emoticon', 'Hashtag']\n",
      "# We assume that we've not included addressees\n",
      "W_U = W_noisy[:,dictStarts['URL']:dictEnds['URL']]\n",
      "W_T = W_noisy[:,dictStarts['Text']:dictEnds['Text']]\n",
      "W_S = W_noisy[:,dictStarts['Stock']:dictEnds['Stock']]\n",
      "W_E = W_noisy[:,dictStarts['Emoticon']:dictEnds['Emoticon']]\n",
      "W_H = W_noisy[:,dictStarts['Hashtag']:dictEnds['Hashtag'] - 1]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "d[dictStarts['Hashtag'] - 1], d[dictStarts['Hashtag']],d[dictEnds['Hashtag'] - 1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "('\u1f49B', '#dfmchat', '#roadtomerion')"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Unified Design Matrix"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Y = ssp.hstack([X_U, X_W, X_M, W_U, W_T, W_S, W_E, W_H]).tocsr()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Y.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "(603652, 80191)"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Noisy Design Matrix"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "flip_count = 300\n",
      "flippedRows = rd.choice(Y.shape[0], flip_count) # Samples w/o replacement\n",
      "flippedCols = np.ndarray(shape=(flip_count,), dtype=np.int32)\n",
      "flipped_neg_to_pos = flippedRows[:flip_count//2]\n",
      "flipped_pos_to_neg = flippedRows[flip_count//2:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Positive to Negative\n",
      "Z = Y.copy()\n",
      "c = 0\n",
      "for row in flipped_pos_to_neg:\n",
      "    col = rd.choice(Z[row,:].indices)\n",
      "    \n",
      "    flippedCols[c] = col\n",
      "    c += 1\n",
      "    Z[row,col] = 0\n",
      "\n",
      "# Expect many warnings here about the cost of sparsity altering changes\n",
      "for row in flipped_neg_to_pos:\n",
      "    alreadyPositiveCols = Z[row,:].indices\n",
      "    \n",
      "    col = rd.randint(Z.shape[1])\n",
      "    while col in alreadyPositiveCols:\n",
      "        col = rd.randint(Z.shape[1])\n",
      "    \n",
      "    flippedCols[c] = col\n",
      "    c += 1\n",
      "    Z[row, col] = 1\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/opt/local/Library/Frameworks/Python.framework/Versions/3.3/lib/python3.3/site-packages/scipy/sparse/compressed.py:728: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
        "  SparseEfficiencyWarning)\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "flipped = list(zip(flippedRows,flippedCols))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Matrix Factorization"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.decomposition import NMF, ProjectedGradientNMF"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "K                   = 10       # Number of components, analgous to latent topics in TMs\n",
      "init                = 'nndsvd' # Various SVD methods available which are better\n",
      "sparseness          = 'data'\n",
      "degreeOfSparsity    =  0.6 # Larger values mean more sparse. Only if sparseness != None\n",
      "degreeOfCorrectness =  0.1 # Smaller values mean larger error. Only if sparseness != None\n",
      "tolerance           = 1E-4 # Stopping condition\n",
      "max_iters           =  100\n",
      "nls_max_iters       =  500\n",
      "randomState         = None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model = ProjectedGradientNMF(\\\n",
      "    n_components = K, \\\n",
      "    init         = init, \\\n",
      "    sparseness   = sparseness, \\\n",
      "    beta         = degreeOfSparsity, \\\n",
      "    eta          = degreeOfCorrectness, \\\n",
      "    tol          = tolerance, \\\n",
      "    max_iter     = max_iters, \\\n",
      "    nls_max_iter = nls_max_iters, \\\n",
      "    random_state = randomState \\\n",
      ")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Z_sub = Z[1:10000,:]\n",
      "W = model.fit_transform(Z_sub)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle as pkl"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('/Users/bryanfeeney/Desktop/nmf_model_2.pkl', 'wb') as f:\n",
      "    pkl.dump((W,model), f)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "H = model.components_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 51
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model.reconstruction_err_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 52,
       "text": [
        "353.4299586004461"
       ]
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "H.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 53,
       "text": [
        "(10, 80191)"
       ]
      }
     ],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "W.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 54,
       "text": [
        "(9999, 10)"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Z_sub_recons = W.dot(H)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Z_sub[0].indices"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 57,
       "text": [
        "array([    0,   368,   386,  7724, 11184, 26021, 51296, 54718, 57049,\n",
        "       59092, 62856, 72593, 79181], dtype=int32)"
       ]
      }
     ],
     "prompt_number": 57
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.where(Z_sub_recons[0] > 0.1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 60,
       "text": [
        "(array([  5,  22, 364, 365, 366, 367, 386, 387, 388]),)"
       ]
      }
     ],
     "prompt_number": 60
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Z_sub[30].indices"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 61,
       "text": [
        "array([    0,   367,   386,  3089,  6018,  6491,  6928, 10406, 13692,\n",
        "       23872, 30957, 35200, 47892, 49270, 54188, 55990, 59799, 60147], dtype=int32)"
       ]
      }
     ],
     "prompt_number": 61
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.where(Z_sub_recons[30] > 0.1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 62,
       "text": [
        "(array([ 31, 364, 365, 366, 367, 386]),)"
       ]
      }
     ],
     "prompt_number": 62
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Z_sub[9000].indices"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 64,
       "text": [
        "array([   30,   370,   387,  2790, 41929, 44886, 45635, 55331, 70793], dtype=int32)"
       ]
      }
     ],
     "prompt_number": 64
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.where(Z_sub_recons[9000] > 0.1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 65,
       "text": [
        "(array([  5, 369, 370, 371, 372, 387]),)"
       ]
      }
     ],
     "prompt_number": 65
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}